{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b166a9c0-4b54-4255-8e35-5e587732627e",
   "metadata": {},
   "source": [
    "# N2D to TIF\n",
    "\n",
    "This notebook uses the library [**ND2**](https://github.com/tlambert03/nd2) to read the Nikon ND2 files as numpy arrays and then save them as 'TIF' files.\n",
    "\n",
    "With [**ND2**](https://github.com/tlambert03/nd2), you can either read the ND2 file to a numpy array (`nd2.imread`) or use `nd2.ND2File()` as a context manager to get the attributes and metadata.\n",
    "\n",
    "Additional libraries that you may need to install to run this notebook:\n",
    "* [skimage](https://scikit-image.org/): This library is used for the `downsampling` functions.\n",
    "* [tifffile](https://github.com/cgohlke/tifffile/tree/master): To save the ND2 recordings as TIF files.\n",
    "\n",
    "Other libraries that read Nikon files include: \n",
    "* [N2dreader](https://github.com/Open-Science-Tools/nd2reader)\n",
    "* [AICSImageIO](https://allencellmodeling.github.io/aicsimageio/): `pip install aicsimageio[nd2]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1907d8-0de1-4cae-b5a3-3879910cec73",
   "metadata": {},
   "source": [
    "# Example data\n",
    "\n",
    "In vivo recordings of sequential widefield imaging (`NIK01a01_seq`), single-channel widefield imaging (`NIK02a01`), and two-photon microscopy (`NIK01b01`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf4c6e-b72f-475c-8e6f-0500b184ba26",
   "metadata": {},
   "source": [
    "# Import the libraries\n",
    "\n",
    "Versions for this notebook: NumPy 1.23.5, Pandas 1.5.3, scikit-image 0.24.0, Matplotlib 3.6.3, nd2 0.10.1, tifffile 2023.2.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e607b2a-58aa-4efb-9249-6ce28438201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Nikon files\n",
    "import nd2\n",
    "import tifffile\n",
    "\n",
    "# Metadata\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom.minidom import parseString\n",
    "\n",
    "# Downsampling function\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0edcc-8931-4f46-97e3-79b5c2115027",
   "metadata": {},
   "source": [
    "# Create the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e08b6-20ad-4151-8f07-8406ccbf3ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_name = 'nd2_to_tif'\n",
    "\n",
    "# Data path to 'Data_example' folders. Change accordingly to your data structure.\n",
    "data_path = os.path.dirname(os.getcwd())  # Moves one level up from the current directory\n",
    "\n",
    "# Change the folder names accordingly\n",
    "paths = {'data': data_path,\n",
    "         'raw_data':  f'{data_path}/Data_examples/{notebook_name}/',\n",
    "         'processed_data': f'{data_path}/Processed_data_examples/{notebook_name}/',\n",
    "         'analysis': f'{data_path}/Analysis_examples/{notebook_name}/',         \n",
    "         'plots': f'{data_path}/Analysis_examples/{notebook_name}/Plots/'}\n",
    "\n",
    "# Make folders if they do not exist yet\n",
    "for path in paths.values():\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5651857f-013c-401e-852a-8e8fa26507d8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252c1c5-b600-4dff-9800-b8fd30059970",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "\n",
    "The original downsample function was written by Hidde Damen (MSc student, 2023) using the `skimage` transform function [Scikit-image](https://scikit-image.org/docs/stable/auto_examples/transform/plot_rescale.html) based on the factor provided. I added the following modifications: \n",
    "- [Skimage rescale_intensity](https://scikit-image.org/docs/stable/api/skimage.exposure.html#skimage.exposure.rescale_intensity) was removed because it changes the distribution of pixel intensity values.\n",
    "- Processing in blocks is useful when dealing with large stacks or memory limitations. Within each block iteration, the `del` statement releases memory for processed frames that are no longer needed and may help reduce memory usage further.\n",
    "\n",
    "Read here about the following [image data types](https://scikit-image.org/docs/dev/user_guide/data_types.html):\n",
    "\n",
    "\n",
    "|Data type|Range|\n",
    "|--|--|\n",
    "|uint8|0 to 255|\n",
    "|unit16|0 to 65535|\n",
    "|uint32|0 to 2^^32^ - 1|\n",
    "|float|-1 to 1 or 0 to 1|\n",
    "|int8|-128 to 127|\n",
    "|int16|-32768 to 32767|\n",
    "|int32|-2^^32^ to 2^^32^ - 1|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd2989-7e99-48ed-ba34-e6d2bc31b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_stack(stack, block_size, nd2_downsample_factor, dtype=np.uint16):\n",
    "    \"\"\"\n",
    "    Downsamples a stack in the XY plane, processing the stack in blocks of frames.\n",
    "    Parameters:\n",
    "        stack: the input stack\n",
    "        block_size: Number of frames to process in each block. Note: Use the length of the stack for processing in one block.\n",
    "        nd2_downsample_factor: the factor with which to downsample in the X and Y direction\n",
    "        dtype: the dtype of the returned stack, defaults to np.uint16\n",
    "    Returns the new stack, where frames are downsampled and blocks are preserved.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_frames = stack.shape[0]  # Number of frames in the stack\n",
    "    shape = list(stack.shape)  # Shape of the stack\n",
    "    shape[-2] = int(shape[-2] / nd2_downsample_factor)  # Update the shape to downsample in X \n",
    "    shape[-1] = int(shape[-1] / nd2_downsample_factor)  # Update the shape to downsample in Y \n",
    "    downsampled_stack = np.empty(shape, dtype=dtype)  # Create an empty array for the new stack\n",
    "\n",
    "    # Iterate over blocks of frames\n",
    "    for block_start in range(0, num_frames, block_size):\n",
    "        block_end = min(block_start + block_size, num_frames)  # Calculate the end index of the current block\n",
    "        frames_block = stack[block_start:block_end]  # Get the frames within the current block\n",
    "\n",
    "        # Downsample frames \n",
    "        shape = list(np.shape(frames_block))  # Get the total number of frames in the stack\n",
    "        shape[-2] = int(shape[-2] / nd2_downsample_factor)  \n",
    "        shape[-1] = int(shape[-1] / nd2_downsample_factor) \n",
    "        \n",
    "        # Resize downsampled blocks of frames\n",
    "        downsampled_block = resize(frames_block, shape, \n",
    "                                   order=0, preserve_range=True,  \n",
    "                                   anti_aliasing=True)  # Default: True\n",
    "        downsampled_block = downsampled_block.astype(dtype)\n",
    "\n",
    "        # Store the downsampled frames in the new stack\n",
    "        downsampled_stack[block_start:block_end] = downsampled_block.astype(dtype)\n",
    "\n",
    "        # Delete the processed block from memory\n",
    "        del frames_block  \n",
    "\n",
    "    return downsampled_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d452482-a601-4290-9e85-f14b46463737",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "* [ND2 documentation](https://github.com/tlambert03/nd2)\n",
    "* Nikon file dimensions: {'T', 'Z', 'Channel', 'Y', 'X'}\n",
    "* Be aware that the camera and Nikon software may not pass an accurate time-stamp of the images. The timestamp NIS shows is the moment that the image data arrived at the PC (software jitter). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b14e8-aac3-491f-8c87-b0fe594ee271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nd2_metadata(experiment, recording, paths):\n",
    "    \"\"\"\n",
    "    Extracts metadata from ND2 files and returns it as dictionaries.\n",
    "    Args:\n",
    "        experiment (str): The name of the experiment.\n",
    "        recording (str): The name of the recording.\n",
    "        paths (dict): Dictionary with paths.\n",
    "    Returns:\n",
    "        attributes_dict (dict): A dictionary containing metadata attributes extracted from the ND2 file.\n",
    "        metadata_text (str): Textual metadata information from the ND2 file.\n",
    "    \"\"\"\n",
    "    # Construct the path to the ND2 file\n",
    "    nd2_file_path = f\"{paths['raw_data']}{experiment}/{recording}.nd2\"\n",
    "    \n",
    "    # Extract metadata from the ND2 file\n",
    "    with nd2.ND2File(nd2_file_path) as nd2file:\n",
    "        # Create a dictionary to store metadata attributes\n",
    "        attributes_dict = {}\n",
    "        attributes_dict['duration_s'] = nd2file.events()[-1]['Time [s]'] - nd2file.events()[0]['Time [s]']\n",
    "        attributes_dict['sizes'] = nd2file.sizes\n",
    "        attributes_dict['voxel_size'] = nd2file.voxel_size()\n",
    "        attributes_dict['dtype'] = nd2file.dtype\n",
    "        attributes_dict['attributes'] = nd2file.attributes\n",
    "        \n",
    "        # Extract metadata text\n",
    "        metadata_text = nd2file.text_info\n",
    "    \n",
    "    # Return the attributes dictionary and metadata text\n",
    "    return attributes_dict, metadata_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763d346-4684-40a7-8462-25880692bb0e",
   "metadata": {},
   "source": [
    "# Loop through files\n",
    "\n",
    "**Note**: This is an example to loop through the different ND2 example recordings. Adapt the if statements to your data structure and filenames. \n",
    "\n",
    "The loop uses the [walk function](https://docs.python.org/3/library/os.html) that yields a directory tree:\n",
    "* `dirpath`. String, the path to the directory.\n",
    "* `dirnames`. List of subdirectories\n",
    "* `filenames`. List of file names. \n",
    "\n",
    "Other types of recordings can also be processed or filtered by adding more if statements or [regular expressions](https://docs.python.org/3/library/re.html) (see [cheat sheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/)). For example, you can save an Excel or CSV file with annotations in your raw data folder, containing the acquisition mode: 'xyt', 'xyt_seq', etc. It reads the annotations file, and the if statements process the Nikon files accordingly, using the information associated with each recording.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616e2092-7735-41c9-ad98-572f1f65bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample parameters (change as needed)\n",
    "block_size = 500\n",
    "downsample_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794c172-a9c7-41d8-a2bb-fbc9bb44d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to check if any .nd2 files are found\n",
    "nd2_files_found = False\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(paths['raw_data']):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.nd2'):\n",
    "            nd2_files_found = True  # Update flag if a file is found\n",
    "\n",
    "            # Paths and names\n",
    "            experiment = os.path.split(dirpath)[1]\n",
    "            recording = os.path.splitext(filename)[0]\n",
    "            file_path = f\"{dirpath}/{filename}\"\n",
    "            tif_save_path = f\"{paths['processed_data']}{experiment}\"\n",
    "\n",
    "            # Create folders\n",
    "            os.makedirs(tif_save_path, exist_ok=True)\n",
    "            os.makedirs(f\"{paths['processed_data']}{experiment}/Metadata\", exist_ok=True)\n",
    "\n",
    "            # Load the stack as numpy array\n",
    "            stack = nd2.imread(file_path)\n",
    "\n",
    "            # Extract and save ND2 files metadata\n",
    "            attributes, metadata = nd2_metadata(experiment, recording, paths)\n",
    "            # Save the attributes dictionary and metadata as text files\n",
    "            with open(f\"{paths['processed_data']}{experiment}/Metadata/{recording}_attributes.txt\", 'w') as attributes_file:\n",
    "                for key, value in attributes.items():\n",
    "                    attributes_file.write(f\"{key}: {value}\\n\")\n",
    "            with open(f\"{paths['processed_data']}{experiment}/Metadata/{recording}_metadata.txt\", 'w') as metadata_file:\n",
    "                for key, value in metadata.items():\n",
    "                    metadata_file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "            with nd2.ND2File(file_path) as nd2file:\n",
    "                \n",
    "                # Time-series with only one channel\n",
    "                if 'C' not in nd2file.sizes and nd2file.sizes.get('T', 0) > 1 and not filename.endswith('_seq.nd2'):\n",
    "\n",
    "                    # Downsample the stack\n",
    "                    stack = downsample_stack(stack, block_size, downsample_factor)\n",
    "                    \n",
    "                    # Save the downsampled stack as tif\n",
    "                    new_height = stack.shape[-2]\n",
    "                    save_path_file = f\"{tif_save_path}/{recording}_{new_height}px.tif\"\n",
    "                    tifffile.imwrite(save_path_file, stack)\n",
    "                    print(save_path_file)\n",
    "\n",
    "                # Time-series with sequential channels recorded as one channel\n",
    "                if 'C' not in nd2file.sizes and nd2file.sizes.get('T', 0) > 1 and filename.endswith('_seq.nd2'):\n",
    "\n",
    "                    # Downsample the stack\n",
    "                    stack = downsample_stack(stack, block_size, downsample_factor)\n",
    "                    \n",
    "                    # Separate channels \n",
    "                    ch1 = stack[0::2, :, :]\n",
    "                    ch2 = stack[1::2, :, :]\n",
    "                    \n",
    "                    # Save the downsampled stack as separate tif files\n",
    "                    new_height = stack.shape[-2]\n",
    "                    save_path_ch1 = f\"{tif_save_path}/{recording}_ch1_{new_height}px.tif\"\n",
    "                    save_path_ch2 = f\"{tif_save_path}/{recording}_ch2_{new_height}px.tif\"\n",
    "                    tifffile.imwrite(save_path_ch1, ch1)\n",
    "                    tifffile.imwrite(save_path_ch2, ch2)\n",
    "                    print(save_path_ch1), print(save_path_ch2)\n",
    "\n",
    "                # Time-series with two channels\n",
    "                if nd2file.sizes.get('C', 0) == 2 and nd2file.sizes.get('T', 0) > 1:\n",
    "\n",
    "                    # Downsample the stack (if needed)\n",
    "                    # stack = downsample_stack(stack, block_size, downsample_factor)\n",
    "                    \n",
    "                    # Separate channels \n",
    "                    ch1 = stack[:, 0, :, :]\n",
    "                    ch2 = stack[:, 1, :, :]\n",
    "                    \n",
    "                    # Save the downsampled stack as separate tif files\n",
    "                    new_height = stack.shape[-2]\n",
    "                    save_path_ch1 = f\"{tif_save_path}/{recording}_ch1.tif\"\n",
    "                    save_path_ch2 = f\"{tif_save_path}/{recording}_ch2.tif\"\n",
    "                    tifffile.imwrite(save_path_ch1, ch1)\n",
    "                    tifffile.imwrite(save_path_ch2, ch2)\n",
    "                    print(save_path_ch1), print(save_path_ch2)\n",
    "\n",
    "if not nd2_files_found:\n",
    "    print(\"No 'nd2' files were found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a260228-e97e-4b22-ad1d-9199b1b17cfb",
   "metadata": {},
   "source": [
    "# Histograms\n",
    "\n",
    "To identify artifacts and possible changes in image intensity values after downsampling, plot the cumulative histograms to compare the pixel distribution before and after downsampling. In this example, I used the first channel of the stack.\n",
    "\n",
    "See options for plotting [Cumulative histograms](https://matplotlib.org/stable/gallery/statistics/histogram_cumulative.html) in Matplotlib.\n",
    "\n",
    "**Note**: Here I show another way to loop through specific files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27511c4-78ee-4665-9908-e1202f8903ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = ['NIK01', 'NIK02']\n",
    "recordings = ['NIK01b01', 'NIK02a01']\n",
    "\n",
    "# Create a dictionary with the selected recordings\n",
    "recordings_dict = defaultdict(list)\n",
    "for recording in recordings:\n",
    "    for experiment in experiments:\n",
    "        if recording.startswith(experiment):\n",
    "            recordings_dict[experiment].append(recording)\n",
    "\n",
    "for experiment, recording_list in recordings_dict.items():\n",
    "    for recording in recording_list: \n",
    "        # Load original stacks\n",
    "        stack_orig_path = glob.glob(f\"{paths['raw_data']}{experiment}/{recording}*.nd2\")[0]\n",
    "        \n",
    "        stack_orig = nd2.imread(stack_orig_path)\n",
    "        \n",
    "        # # Load downsampled stacks\n",
    "        stack_down_path = glob.glob(f\"{paths['processed_data']}{experiment}/{recording}*_*.tif\")[0]\n",
    "        stack_down = tifffile.imread(stack_down_path)\n",
    "        \n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(stack_orig[:, :, 0].flatten(), density=True, bins=100, \n",
    "                cumulative=True, histtype='step', fill=False, \n",
    "                edgecolor='gray', label='original')\n",
    "        ax.hist(stack_down[:, :, 0] .flatten(), density=True, bins=100, \n",
    "                cumulative=True, histtype='step', fill=False, \n",
    "                edgecolor='magenta', label='downsampled')\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{recording}\")\n",
    "        ax.set_xlabel(\"Pixel Values\")\n",
    "        ax.set_ylabel(\"Cumulative Density\")\n",
    "    \n",
    "        # Save plots\n",
    "        os.makedirs(f\"{paths['plots']}{experiment}\", exist_ok=True)\n",
    "        plot_save_path = f\"{paths['plots']}{experiment}/{recording}\"\n",
    "        plt.savefig(f\"{plot_save_path}_histogram.png\", dpi=300)\n",
    "        print(f\"{plot_save_path}_histogram.png\")\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
