{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701849e5-7c03-46c6-a989-227c1c12be49",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tables and plots\n",
    "\n",
    "Python is a powerful tool for exploring data stored in **DataFrames** using the [Pandas](https://pandas.pydata.org/) library. In this notebook, I show a few basic examples of how to navigate tables with Pandas, plot data using [Matplotlib](https://matplotlib.org/) and [Seaborn](https://seaborn.pydata.org/), and perform statistical tests using [SciPy](https://docs.scipy.org/doc/scipy/reference/stats.html). For learning more, I highly recommend diving into any of the linked resources.\n",
    "\n",
    "\n",
    "**Genral resources**\n",
    "* [Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/) by Jonas Kristoffer Lindeløv.\n",
    "* [Learning statistics with Python](https://ethanweed.github.io/pythonbook/landingpage.html) by Danielle Navarro and Ethan Wood.\n",
    "* [Neural Data Science in Python](https://neuraldatascience.io/intro.html) by Aaron J Newman.\n",
    "* [Pandas Cookbook](https://github.com/jvns/pandas-cookbook) by Jake VanderPlas.\n",
    "* [Pandas Notebooks](https://github.com/plembo/pandas-tutorials) by Corey Schafer's.\n",
    "* [The Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake VanderPlas. See [Visualization with Matplotlib](https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html), [Visualization with Seaborn](https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html), and [Data Manipulation with Pandas](https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d64d2-bd3a-4194-a07e-c045f14b7da8",
   "metadata": {},
   "source": [
    "# Example data\n",
    "\n",
    "In this notebook, I will use the brain cell database from the **Allen Institute for Brain Science** as an example dataset.  You can also download the table directly from the [Allen Brain Atlas website](https://celltypes.brain-map.org/data) by clicking **\"Download Cell Feature Data.\"** This dataset contains electrophysiological and morphological data obtained from patch-clamp recordings in both mouse and human brains. These features were used to classify neurons into distinct subtypes (**Figure 1**), which is one of the main challenges in neuroscience because of the [brain’s high cellular diversity](https://spikesandbursts.wordpress.com/2021/02/28/mapping-neuronal-diversity/).  \n",
    "\n",
    "Defining a cell type is not trivial, and morphoelectrical features can be combined with transcriptomic data for a more integrative classification ([Gouwens et al., 2020](https://pubmed.ncbi.nlm.nih.gov/33186530/), [Scala et al., 2021](https://www.nature.com/articles/s41586-020-2907-3)). The goal is to identify cell types with similar attributes and functions. For example, although **inhibitory interneurons** ([Tremblay et al., 2016](https://www.sciencedirect.com/science/article/pii/S0896627316303117)) are fewer than excitatory neurons in the cortex (about 20% vs. 80%), there are at least four major classes of inhibitory neurons with distinct intrinsic and functional properties: Pvalb, Vip, Sst, and Lamp5 (**Figure 1b**).  \n",
    "\n",
    "\n",
    "<img src=\"../Figures/neurons_morpho-electric_types_gouwens_et_al_2019.png\" width=\"1200\"/>\n",
    "\n",
    "**Figure 1.** Examples of excitatory (a) and inhibitory (b) neurons in the mouse visual cortex. Top panels: morphological reconstructions of dendrites.  Bottom panels: electrophysiological responses from the same neurons to hyperpolarizing and depolarizing current injection.  Source: [Gouwens et al., 2019](https://pmc.ncbi.nlm.nih.gov/articles/PMC8078853/). \n",
    "\n",
    "\n",
    "You can explore the Allen Institute Cell Database using the [interactive website](https://celltypes.brain-map.org/data) or the [Allen Software Development Kit (SDK)](https://allensdk.readthedocs.io/en/latest/).  Through the Allen SDK, you can access all available electrophysiology measurements and morphological reconstructions (see links below).\n",
    "\n",
    "**Additional resources and reading**\n",
    "* [Cell Types Database](https://alleninstitute.github.io/AllenSDK/_static/examples/nb/cell_types.html). Example notebooks and documentation.  \n",
    "* [Open Neuroscience Education](https://sites.google.com/ucsd.edu/neuroedu/cell-types/jupyter-instructions). Excellent teaching resources created by Dr. Ashley Juavinett.  \n",
    "* [Transgenic mouse lines](https://portal.brain-map.org/explore/toolkit/mice). The Allen Institute for Brain Science used [Cre mouse lines](https://spikesandbursts.wordpress.com/2018/07/18/cre-mice-for-targeting-neurons/) to identify genetically defined cell classes. Driver lines are used to target specific populations, while reporter lines are used to visualize (with fluorescent proteins) or manipulate those populations.\n",
    "* [Zheng et al., 2022](https://www.sciencedirect.com/science/article/pii/S0092867422007838). What is a cell type and how to define it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211c625-a587-413b-b133-bb93f8b8e4e5",
   "metadata": {},
   "source": [
    "# Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096a518-06cb-46c6-8340-4d65e90d15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Optional: Display all columns in table\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display all or a specific number of rows\n",
    "# pd.set_option('display.max_rows', 20)\n",
    "\n",
    "# Stats libraries\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro, bartlett\n",
    "\n",
    "# Plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Interactive plots (comment out if needed)\n",
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "# #For Jupyter Lab:\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b9f0f4-538a-4be4-b986-d6536f030f1c",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ae2be-68ae-4286-ae0a-c6b9396ced4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the paths and folder names according to your data structure\n",
    "notebook_name = 'dataframes_plots'\n",
    "\n",
    "# Data path to 'Data_example' folders. Change accordingly to your data structure.\n",
    "data_path = os.path.dirname(os.getcwd())  # Moves one level up from the current directory\n",
    "\n",
    "# Change the folder names accordingly\n",
    "paths = {'data': data_path,\n",
    "         'raw_data':  f'{data_path}/Data_examples/{notebook_name}/',\n",
    "         'processed_data': f'{data_path}/Processed_data_examples/{notebook_name}/',\n",
    "         'analysis': f'{data_path}/Analysis_examples/{notebook_name}/',         \n",
    "         'plots': f'{data_path}/Analysis_examples/{notebook_name}/Plots/'}\n",
    "\n",
    "# Make folders if they do not exist yet\n",
    "for path in paths.values():\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55033be-3c1a-4f84-a53d-a03f13067bb9",
   "metadata": {},
   "source": [
    "# Plot settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2a837-1830-4d8d-840b-ae123da398f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib settings\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.size': 18,  # Base size used by most elements\n",
    "    'axes.labelsize': 18,     # Axis labels\n",
    "    'axes.titlesize': 18,     # Plot titles\n",
    "    'xtick.labelsize': 18,    # X axis numbers\n",
    "    'ytick.labelsize': 18,    # Y axis numbers\n",
    "    'legend.fontsize': 18,    # Legend text\n",
    "    'savefig.transparent': True,\n",
    "    'svg.fonttype': 'none',   # Editable text in SVGs\n",
    "})\n",
    "\n",
    "# Seaborn settings\n",
    "sns.set(style=\"ticks\",  \n",
    "        context=\"notebook\", \n",
    "        font=\"Arial\",\n",
    "        rc={\n",
    "            \"axes.labelsize\": 18,\n",
    "            \"axes.titlesize\": 18,\n",
    "            \"xtick.labelsize\": 18,\n",
    "            \"ytick.labelsize\": 18,\n",
    "            \"legend.fontsize\": 18,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49e649-3875-4453-9eb9-d062190a3d12",
   "metadata": {},
   "source": [
    "# Load the dataframe\n",
    "\n",
    "* For excel files you can use [`pandas.read_excel`](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html).\n",
    "* For CSV files, you can use [`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b577d1-4747-401e-b02a-a6ebbda1a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(os.path.join(paths['raw_data'], 'cell_types_specimen_details.csv'))  # or pd.read_csv(paths['raw_data'] + '/cell_types_specimen_details.csv')\n",
    "\n",
    "dataset.head(10)  # Use head or tail to show the first n rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c152f-2e15-44c3-b0a8-6c0c99e9cb4d",
   "metadata": {},
   "source": [
    "# Merge dataset\n",
    "\n",
    "In this example, the data is already in one table, but I give a mock example of how to join tables with different parameters of the same recordings using [`pandas.merge`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html). You can also combine tables using the index of DataFrames with [`pandas.join`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b216a043-dfb2-4f2e-bb78-576149e2f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock example splitting the dataset and joining it again\n",
    "dataset = pd.read_csv(os.path.join(paths['raw_data'], 'cell_types_specimen_details.csv'))\n",
    "\n",
    "# Split into two subsets, keeping 'specimen__id' in both\n",
    "dataset_subset1 = dataset[['specimen__id'] + list(dataset.columns[2:10])]\n",
    "dataset_subset2 = dataset[['specimen__id'] + list(dataset.columns[11:20])]\n",
    "\n",
    "# Save subsets to the analysis folder\n",
    "dataset_subset1.to_csv(os.path.join(paths['analysis'], 'dataset_subset1.csv'), index=False)\n",
    "dataset_subset2.to_csv(os.path.join(paths['analysis'], 'dataset_subset2.csv'), index=False)\n",
    "\n",
    "# Join the subsets on the common column 'specimen__id'\n",
    "dataset_merged = pd.merge(dataset_subset1, dataset_subset2, on='specimen__id')\n",
    "\n",
    "# Save the merged dataframe\n",
    "dataset_merged.to_csv(os.path.join(paths['analysis'], 'dataset_merged.csv'), index=False)\n",
    "dataset_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f8f9e-f930-45a2-8b67-c36ae1880886",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Concatenate tables\n",
    "\n",
    "Sometimes you may have one table per recording. Here is an example showing how to concatenate them into a single table using a mock example by splitting the current table. The same approach applies to separate tables as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef175f-5ece-4322-acc8-43059174ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock example: splitting the dataset by rows\n",
    "dataset = pd.read_csv(os.path.join(paths['raw_data'], 'cell_types_specimen_details.csv'))\n",
    "\n",
    "recordings_1 = dataset.iloc[:100, :]  \n",
    "recordings_2 = dataset.iloc[100:200, :]  \n",
    "\n",
    "# Save the subsets to the analysis folder\n",
    "recordings_1.to_csv(os.path.join(paths['analysis'], 'recordings_1.csv'), index=False)\n",
    "recordings_2.to_csv(os.path.join(paths['analysis'], 'recordings_2.csv'), index=False)\n",
    "\n",
    "# Loop through the recording tables\n",
    "recordings_tables = []\n",
    "\n",
    "for filename in os.listdir(paths['analysis']):\n",
    "    if filename.startswith('recordings'):\n",
    "        recording_table = pd.read_csv(os.path.join(paths['analysis'], filename))\n",
    "        recordings_tables.append(recording_table)\n",
    "\n",
    "# Concatenate all recording tables\n",
    "recordings_concatenated = pd.concat(recordings_tables, ignore_index=True)\n",
    "\n",
    "# Save the merged table\n",
    "recordings_concatenated.to_csv(os.path.join(paths['analysis'], 'recordings_concatenated.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c9703-7005-48b4-90d7-0c8fe626d839",
   "metadata": {},
   "source": [
    "# Transform the table\n",
    "\n",
    "Once you have merged the dataset, you may still need to transform it to add calculations or new labels. Common transformations include:\n",
    "\n",
    "- Transpose the dataframe: `dataframe.T`\n",
    "- Insert a column: `dataframe.insert` or `dataframe.assign`, `insert` adds a new column at a specific position in a copied dataframe, while `assign` creates a new column without changing the original dataframe unless you save it.\n",
    "- Rename a column: `dataframe.rename`\n",
    "\n",
    "It is advisable to work on a copy of the table and, when saving, use a different name to avoid overwriting the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e362bec-e99b-4d24-92c2-c413de36b624",
   "metadata": {},
   "source": [
    "## Insert a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57b38a-c90c-417b-8bd8-df8b28941045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a new column\n",
    "dataset_processed = dataset.copy()  # Make a copy of the original dataset\n",
    "dataset_processed.insert(0, 'id', range(1, len(dataset_processed) + 1))\n",
    "dataset_processed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f8286-79c6-4516-88c9-8950fa8200b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.assign(id=range(1, len(dataset) + 1))\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f864e8be-283d-4c18-9f5e-8ab826744771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original dataset has not been changed\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ac925-46e4-4fea-8747-39e94006b51a",
   "metadata": {},
   "source": [
    "## Rename columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0972359b-8d16-47df-b641-a9213b286347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a column\n",
    "dataset_processed = dataset.copy()  # Make a copy of the original dataset\n",
    "dataset_processed = dataset.rename(\n",
    "    columns={\"structure__name\": \"brain_structure\"})\n",
    "\n",
    "dataset_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e3d47-a48c-4d7d-a1d3-f96e782d1f91",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e4599-b6d0-4628-b37a-9205d886651a",
   "metadata": {},
   "source": [
    "Here are some Pandas functions that are useful for getting an overview of large datasets. You can also type `?` after each function in Python for a longer explanation.\n",
    "\n",
    "* `dataset.shape`. Shows the number of rows and columns (also displayed at the bottom of the table).\n",
    "* `dataset.columns`. Lists the names of all columns.\n",
    "* `dataset.info()`. Displays all columns, the number of non-null values, and the data type of each column.\n",
    "* `dataset.head()`. Returns the first few rows (5 by default).\n",
    "* `dataset.tail()`. Returns the last few rows (5 by default).\n",
    "* `dataset.loc[rows, [columns]]`. Select specific rows and columns by label.\n",
    "* `dataset.sort_values(by='column_name')`. Sorts the dataframe by a specific column (ascending by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c9562-270d-4f12-a2ab-23d4e122eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique structures in the dataset\n",
    "dataset.loc[0:5, ['structure__name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c512f-be73-43ca-b0ea-db4d8dc1d16c",
   "metadata": {},
   "source": [
    "## Filter the table\n",
    "\n",
    "Filtering the table is not strictly necessary, since you can select specific rows and columns for analysis. However, it can be convenient for many things. For example, showing only mouse data. You can then use the functions above to check the sample size of the filtered subset or concatenate many arguments to find the group of interest (see examples in plots and statistics). An important note for beginners to avoid the common Python `KeyError`:\n",
    "\n",
    "* `['column_name']` returns a Pandas Series and can only select one column.\n",
    "* `[['column_name']]` returns a Pandas DataFrame, which can include one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abf5cf-a29e-4fc1-bd07-45ed7e94be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mice = dataset[(dataset['donor__species'] == 'Mus musculus')]\n",
    "dataset_mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfcd467-b922-43e9-8169-acc1f9e48c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mice_ephys = dataset_mice[[column for column in dataset_mice.columns if column.startswith('ef')]]\n",
    "# Option B: same results\n",
    "# dataset_mice_ephys = dataset_mice.loc[:, dataset_mice.columns.str.startswith('ef')]\n",
    "\n",
    "dataset_mice_ephys.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07120171-b161-44df-8c7d-54121c100502",
   "metadata": {},
   "source": [
    "## Aggregate the table\n",
    "\n",
    "Aggregation in Pandas allows you to group data by one or more columns and then compute summary statistics or other calculations for each group, such as counts, means, or sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdfebe1-3525-4ee8-a2a2-f646300c4d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by structure and feature\n",
    "groupby_parameter = 'structure__acronym'\n",
    "ephys_feature = 'ef__avg_firing_rate'\n",
    "\n",
    "grouped_dataset = (\n",
    "    dataset.groupby(groupby_parameter)\n",
    "    .agg(num_cells = (ephys_feature, 'size'), \n",
    "         av_firing_rate = (ephys_feature, 'mean'))\n",
    "    .dropna()  # Ignoring missing values\n",
    "        )\n",
    "\n",
    "grouped_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c6065-d24e-47e8-8239-b3c2a54e2c8c",
   "metadata": {},
   "source": [
    "## Sort the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3e75c-4d88-43d4-8289-42cbbb1dcf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort_values(by='ef__avg_firing_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73acac0b-16b1-4544-84f3-da9e1057fffa",
   "metadata": {},
   "source": [
    "## Summary statistics\n",
    "\n",
    "The function `describe` give you a quick overview of common descriptive statistics. You can use it on the whole dataframe or for specific features. For example:\n",
    "\n",
    "```python\n",
    "dataset.mean(numeric_only=True)             \n",
    "dataset.mean(numeric_only=True).to_frame() # Converts the result into a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d544a3-cac2-419d-aab8-a1e3991d114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics for neuron subtype\n",
    "\n",
    "neuron_subtype = dataset_mice[\n",
    "    (dataset_mice.line_name == 'Pvalb-IRES-Cre') &\n",
    "    (dataset_mice.structure__acronym == 'VISp5')\n",
    "]\n",
    "\n",
    "neuron_subtype.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cf93b-4493-49e9-9429-99dae7b8cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of rows without values (NaN or null)\n",
    "neuron_subtype.isna().sum().to_frame(name='NaN Count').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec483be-0f56-4b6c-b642-6168facb8fc3",
   "metadata": {},
   "source": [
    "## Unique values in columns\n",
    "\n",
    "You can quickly check all unique values in a column using `unique()`, such as neuron subtypes, brain areas, treatments, etc. In this example, we look at all mouse lines used for recordings in the primary visual area by the Allen Brain Institute. Since neurons were recorded in different cortical layers, the handy function [`startswith`](https://www.codecademy.com/resources/docs/python/strings/startswith) and [`endswith`](https://www.codecademy.com/resources/docs/python/strings/endswith) are very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439559b-792c-48df-8121-4cc9714fc5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mice['line_name'][dataset_mice['structure__acronym'].str.startswith('VISp')].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44140e-27de-471a-b87c-9e333eaf019d",
   "metadata": {},
   "source": [
    "# Plots and statistics\n",
    "\n",
    "[Matplotlib](https://matplotlib.org/) and [Seaborn](https://seaborn.pydata.org/) are two powerful libraries for data visualization in Python that allow you to create high-quality figures. While Python may not be as user-friendly as R for statistics, [SciPy](https://docs.scipy.org/doc/scipy/reference/stats.html) and [statsmodels](https://www.statsmodels.org/stable/index.html) do the job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcd10f-2c53-476e-9507-7badddfc8489",
   "metadata": {},
   "source": [
    "## Pandas plots\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) has built-in plotting functions that are easy to use. They are not as versatile as Matplotlib or Seaborn, but are useful for quickly plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244bf12-a6e2-4908-a729-89ffeff967f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_feature = 'ef__tau'\n",
    "\n",
    "feature_means = dataset.groupby('line_name')[ephys_feature].mean()\n",
    "feature_means.plot(kind='bar', figsize=(12,4))\n",
    "\n",
    "ephys_feature = 'ef__tau'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd804720-10d3-46fa-ad70-1075b1946ec5",
   "metadata": {},
   "source": [
    "## Matplotlib: plot + 3-group stats\n",
    "\n",
    "[Matplotlib](https://matplotlib.org/) is one of the most commonly used plotting libraries in Python. It allows you to customize axes, colors, labels, etc. You can combine it with [SciPy](https://docs.scipy.org/doc/scipy/reference/stats.html) to perform statistical analysis.\n",
    "\n",
    "To compare three or more groups, you can use **one-way ANOVA**. This test assumes numeric data, independent samples, normally distributed groups, and homogeneity of variances. If the sample sizes are large enough (normally >25-30), the distribution of the sample means will approximate a Gaussian distribution even if the population itself is not Gaussian (Central Limit Theorem). Normality tests have limited utility in ANOVA for moderate to large samples. If the assumptions of ANOVA are violated, you can use the non-parametric alternative called the [Kruskal-Wallis test](https://www.graphpad.com/guides/prism/latest/statistics/stat_checklist_kw.htm), which does not require normally distributed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8d6db-2dfd-414a-80e8-5abb7df1bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv(os.path.join(paths['raw_data'], 'cell_types_specimen_details.csv'))\n",
    "\n",
    "# Filter dataset for your region of interest\n",
    "filtered_dataset = dataset[\n",
    "    (dataset['donor__species'] == 'Mus musculus') & \n",
    "    (dataset['structure__acronym'] == 'VISp2/3')\n",
    "    # (dataset['structure__acronym'].str.startswith('VISp'))\n",
    "]\n",
    "\n",
    "# Define the groups to compare\n",
    "group_labels = ['Pvalb-IRES-Cre', 'Sst-IRES-Cre', 'Vip-IRES-Cre']\n",
    "\n",
    "# Feature for comparison\n",
    "feature = 'ef__avg_firing_rate'\n",
    "\n",
    "# Extract feature values for each group (drop NaNs)\n",
    "group_data = [\n",
    "    filtered_dataset[filtered_dataset['line_name'] == label][feature].dropna()\n",
    "    for label in group_labels\n",
    "]\n",
    "\n",
    "# Boxplot\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.boxplot(group_data, labels=[lbl.split('-')[0] for lbl in group_labels])\n",
    "ax.set_ylabel(\"Average firing rate (Hz)\")\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig.tight_layout()\n",
    "plot_path = f\"{paths['plots']}VISp_L23_interneurons_{feature}.png\"\n",
    "fig.savefig(plot_path, dpi=300)\n",
    "\n",
    "# Statistics\n",
    "group_stats = pd.DataFrame(columns=['n', 'mean', 'std', 'Shapiro_W', 'Shapiro_p'])\n",
    "\n",
    "# Loop over groups\n",
    "for i, group in enumerate(group_data):\n",
    "    group_stats.at[group_labels[i], 'n'] = len(group)\n",
    "    group_stats.at[group_labels[i], 'mean'] = group.mean()\n",
    "    group_stats.at[group_labels[i], 'std'] = group.std()\n",
    "    \n",
    "    # Shapiro-Wilk test\n",
    "    W, p = shapiro(group)\n",
    "    group_stats.at[group_labels[i], 'Shapiro_W'] = W\n",
    "    group_stats.at[group_labels[i], 'Shapiro_p'] = p\n",
    "\n",
    "# Equality of variances (Bartlett's test)\n",
    "bartlett_stat, bartlett_p = bartlett(group_data[0], group_data[1], group_data[2])\n",
    "p_str = f\"{bartlett_p:.2f}\"\n",
    "print(f\"Bartlett's test: stat = {bartlett_stat:.3f}, p = {p_str}\")\n",
    "\n",
    "# # ANOVA\n",
    "# anova_results = stats.f_oneway(*group_data)\n",
    "# p_str = f\"{anova_results.pvalue:.3f}\" if anova_results.pvalue >= 0.0001 else \"<0.0001\"\n",
    "# print(f\"\\nOne-way ANOVA: F = {anova_results.statistic:.3f}, p = {p_str}\")\n",
    "\n",
    "# Kruskal-Wallis test (non-parametric)\n",
    "kruskal_results = stats.kruskal(group_data[0], group_data[1], group_data[2])\n",
    "p_str = f\"{kruskal_results.pvalue:.3f}\" if kruskal_results.pvalue >= 0.0001 else \"<0.0001\"\n",
    "print(f\"\\nKruskal-Wallis test: H = {kruskal_results.statistic:.3f}, p = {p_str}\")\n",
    "\n",
    "group_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17aad6-d6b7-44f4-a551-532d12299d1f",
   "metadata": {},
   "source": [
    "## Seaborn\n",
    "\n",
    "[Seaborn](https://seaborn.pydata.org/) is built on Matplotlib and offers more flexibility and higher-level plotting functions to create more complex plots with fewer lines. One useful feature is automatic correlation plots (pair plots).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2ca0c-642e-4434-862a-41b3e518a266",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052cc769-25bc-416c-895e-05f7d8aaf0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \n",
    "dataset = pd.read_csv(os.path.join(paths['raw_data'], 'cell_types_specimen_details.csv'))\n",
    "\n",
    "filtered_dataset = (dataset[\n",
    "    (dataset['donor__species'] == 'Mus musculus') & \n",
    "    (dataset['structure__acronym'] == 'VISp2/3')])  # Columns to display between [[]]\n",
    "\n",
    "x_categories = 'line_name'\n",
    "y_feature = 'ef__ri'\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "# Plot the boxplot on the Axes\n",
    "sns.boxplot(x=x_categories, y=y_feature, data=filtered_dataset, ax=ax)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel(x_categories)\n",
    "ax.set_ylabel(y_feature)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plot_path = f\"{paths['plots']}VISp_L23_{x_categories}_{y_feature}.png\"  # or .svg\n",
    "fig.savefig(plot_path, dpi=300)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3845d-72db-4e5e-864d-7760519c8d22",
   "metadata": {},
   "source": [
    "### Plot + 2-group stats\n",
    "\n",
    "Assumptions for t-tests: One variable, numeric data, two groups (or one), random sample, and normally distributed.\n",
    "\n",
    "- GraphPad. [How to test for normality test](https://www.graphpad.com/guides/prism/latest/statistics/stat_how_to_normality_test.htm).\n",
    "- GraphPad. [Ultimate guide to T-tests](https://www.graphpad.com/guides/the-ultimate-guide-to-t-tests).\n",
    "- Learning statistics with Python. [Comparing two-means](https://ethanweed.github.io/pythonbook/05.02-ttest.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78838106-1daf-473a-95eb-7458089e2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset \n",
    "subtype_dataset = dataset[\n",
    "    (dataset['donor__species'] == 'Mus musculus') & \n",
    "    (dataset['structure__acronym'].str.startswith('VISp')) &\n",
    "    (dataset['line_name'] == 'Vip-IRES-Cre')]\n",
    "\n",
    "# Parameters to plot\n",
    "layer_column = 'structure__acronym'\n",
    "layer_L23 = 'VISp2/3'\n",
    "layer_L5 = 'VISp5'\n",
    "\n",
    "feature_column = 'ef__ri'  # Feature to compare (e.g., input resistance)\n",
    "\n",
    "# Apply filters to create subsets\n",
    "L23_subgroup = subtype_dataset[layer_column].eq(layer_L23)\n",
    "L5_subgroup = subtype_dataset[layer_column].eq(layer_L5)\n",
    "\n",
    "group_L23 = subtype_dataset[L23_subgroup][feature_column]\n",
    "group_L5 = subtype_dataset[L5_subgroup][feature_column]\n",
    "\n",
    "# Plot boxplot\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.boxplot([group_L23, group_L5])\n",
    "ax.set_xticklabels([layer_L23, layer_L5])\n",
    "ax.set_ylabel(feature_column)\n",
    "\n",
    "# Create summary table\n",
    "summary_table = pd.DataFrame(index=[layer_L23, layer_L5], columns=['count', 'mean', 'std'])\n",
    "summary_table.loc[layer_L23, 'count'] = group_L23.count()\n",
    "summary_table.loc[layer_L5, 'count'] = group_L5.count()\n",
    "summary_table.loc[layer_L23, 'mean'] = group_L23.mean()\n",
    "summary_table.loc[layer_L5, 'mean'] = group_L5.mean()\n",
    "summary_table.loc[layer_L23, 'std'] = group_L23.std()\n",
    "summary_table.loc[layer_L5, 'std'] = group_L5.std()\n",
    "\n",
    "# Independent T-test\n",
    "ttest_results = stats.ttest_ind(group_L23, group_L5, nan_policy='omit')\n",
    "\n",
    "# Save figure\n",
    "fig.tight_layout()\n",
    "plot_path = f\"{paths['plots']}VISp_L23_5_VIP_{feature_column}.png\"  # or .svg\n",
    "fig.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Results of the T-test:\", ttest_results)\n",
    "# Test normality for each group\n",
    "shapiro_L23 = shapiro(group_L23)\n",
    "shapiro_L5 = shapiro(group_L5)\n",
    "\n",
    "print(f\"Shapiro-Wilk normality test {layer_L23}: W = {shapiro_L23.statistic:.3f}, p = {shapiro_L23.pvalue:.2f}\")\n",
    "print(f\"Shapiro-Wilk normality test {layer_L5}: W = {shapiro_L5.statistic:.3f}, p = {shapiro_L5.pvalue:.2f}\")\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fce309-f57a-40c3-b08d-4d44df1ad62c",
   "metadata": {},
   "source": [
    "### Pairwise correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e82da-ab0b-49d6-acb4-732c9f41baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = dataset[\n",
    "    (dataset['donor__species'] == 'Mus musculus') & \n",
    "    (dataset['structure__acronym'].str.startswith('VISp')) &\n",
    "    (dataset['line_name'] == 'Vip-IRES-Cre')]\n",
    "\n",
    "columns_pairplot = ['ef__f_i_curve_slope',\n",
    "                   'ef__avg_firing_rate',\n",
    "                   'ef__tau',\n",
    "                   'ef__ri']\n",
    "\n",
    "# Create the pair plot\n",
    "pair_plot = sns.pairplot(filtered_dataset[columns_pairplot], diag_kind='hist')\n",
    "\n",
    "# Adjust layout and show\n",
    "pair_plot.fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "plot_path = f\"{paths['plots']}VISp_L23_5_VIP_pairplot.png\"  # or .svg\n",
    "pair_plot.fig.savefig(plot_path, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28d00e-5986-403f-baf9-031e97432418",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "Remember that the **correlation coefficient** measures the strength and direction of a linear relationship between two variables without assuming causality, whereas **regression** assumes a directional relationship and estimates the effect of one variable on another. The Pearson correlation coefficient is commonly denoted as **r**, and its square (**r²**) represents the proportion of variance in the dependent variable that is linearly explained by the independent variable.\n",
    "\n",
    "In the code below, I plot a **regression line** for two electrophysiological features. Here, **r²** is computed from the slope of the regression line and indicates how well the line explains the variability in the data. The associated **p-value** tests whether the slope is significantly different from zero. \n",
    "\n",
    "- GraphPad guide. [The difference between correlation and regression](https://www.graphpad.com/guides/prism/latest/statistics/stat_the_difference_between_correla.htm).\n",
    "- Handbook of Biological Statistics. [Correlation and linear regression](https://www.biostathandbook.com/linearregression.html).\n",
    "- Statistical Thinking for the 21st Century. [Linear regression](https://statsthinking21.github.io/statsthinking21-core-site/the-general-linear-model.html#linear-regression). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27222d9-9050-4af5-b116-2d9f52c9d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset\n",
    "filtered_dataset = dataset[\n",
    "    (dataset['donor__species'] == 'Mus musculus') & \n",
    "    (dataset['structure__acronym'].str.startswith('VISp')) &\n",
    "    (dataset['line_name'] == 'Vip-IRES-Cre')\n",
    "]\n",
    "\n",
    "# Select features for correlation\n",
    "feature_x = filtered_dataset['ef__ri']\n",
    "feature_y = filtered_dataset['ef__vrest']\n",
    "\n",
    "# Fit linear regression and get statistics\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(feature_x, feature_y)\n",
    "\n",
    "# Plot scatter\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(feature_x, feature_y, color='#2ca25f', label='Data points')\n",
    "\n",
    "# Plot regression line\n",
    "ax.plot(feature_x, slope*feature_x + intercept, color='black', linewidth=2, label='Regression line')\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(feature_x.name)\n",
    "ax.set_ylabel(feature_y.name)\n",
    "\n",
    "# Annotate regression equation, R², and p-value\n",
    "ax.text(\n",
    "    0.05, 0.95,\n",
    "    f\"$r^2 = {r_value**2:.3f}$\\n$p = {p_value:.3e}$\",\n",
    "    transform=ax.transAxes,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "\n",
    "# Save figure\n",
    "fig.tight_layout()\n",
    "plot_path = f\"{paths['plots']}VISp_L23_5_VIP_ri_vrest.png\"\n",
    "fig.savefig(plot_path, dpi=300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Optional: print regression results in the notebook\n",
    "print(f\"Regression equation: y = {slope:.3f} * x + {intercept:.3f}\")\n",
    "print(f\"R² = {r_value**2:.3f}\")\n",
    "print(f\"P-value of slope: {p_value:.3e}\")\n",
    "\n",
    "# Compute Pearson correlation coefficient\n",
    "pearson_r = feature_x.corr(feature_y)\n",
    "print(f\"Pearson correlation coefficient (r) = {pearson_r:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
